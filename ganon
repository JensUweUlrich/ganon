#!/usr/bin/python3

import argparse, os, sys, subprocess, io, time, shlex, shutil, gzip, pickle, math
from collections import defaultdict

def main():
    version = '1.0'

    build_parser = argparse.ArgumentParser(description='Build options', add_help=False)

    default_build_group = build_parser.add_argument_group('general')
    default_build_group.add_argument('-i', '--input-files', required=True, nargs="*", type=str, help='Multi-fasta[.gz] file[s]')
    default_build_group.add_argument('-db', '--db-prefix', required=True, type=str, help='Database output prefix (.filter, .nodes, .bins will be created)')
    default_build_group.add_argument('-t', '--threads', type=int, default=1, help='set the number of subprocesses/threads to use for calculations (default: 1)')
    ganon_build_group = build_parser.add_argument_group('ganon')
    ganon_build_group.add_argument('-k', '--kmer-size', type=int, default=19, help='The k-mer size for the bloom filter [14..32] (default: 19)')
    ganon_build_group.add_argument('-n', '--hash-functions', type=int, default=3, help='The number of hash functions to use for the bloom filter [2..5] (default: 3)')
    ganon_build_group.add_argument('-s', '--bloom-size', type=int, default=64, help='Bloom filter size in GB [1..512] [Mutually exclusive --min-fp] (default: 64)')
    ganon_build_group.add_argument('-f', '--min-fp', type=float, help='Min. false positive for bloom filter - slower, requires k-mer counting [Mutually exclusive --bloom-size]')
    #ganon_build_group.add_argument('--max-ram', type=int, default=2, help='Max. available RAM to count k-mers per thread (only with --min-fp) in GB (default: 2)')
    #ganon_build_group.add_argument('--kmc-path', type=str, default='./', help='kmc binaries path (only with --min-fp). Default: ./')
    ganon_build_group.add_argument('--ganon-path', type=str, default='./', help='ganon binaries path. Default: ./')
    taxsbp_build_group = build_parser.add_argument_group('taxsbp')
    taxsbp_build_group.add_argument('-r', '--rank', type=str, default='taxid', help='Lowest taxonomic rank for classification [assembly,taxid,species,genus,...] (Default: taxid)')
    taxsbp_build_group.add_argument('-l', '--bin-length', type=int, default=5000000, help='Maximum length (in bp) for each bin (default: 5000000')
    taxsbp_build_group.add_argument('--fragment-length', type=int, default=-1, help='Fragment length (in bp). Set to 0 to not fragment sequences. (default: bin-length - overlap-length)')
    taxsbp_build_group.add_argument('--overlap-length', type=int, default=500, help='Fragment overlap length (in bp). Should be bigger than the read length used for classificaiton (default: 500)')
    taxsbp_build_group.add_argument('--retrieve-ncbi', default=False, action='store_true', help='Retrieve taxdump (--nodes-file, --merged-file) and TaxSBP information (--len-taxid-file) automatically from NCBI')
    taxsbp_build_group.add_argument('--len-taxid-file', type=str, help='Tab-separated file with the sequence ids, sequence length, taxonomic id and optionally the assembly accession in case of assembly level classification is chosen [Mutually exclusive --retrieve-ncbi]')
    taxsbp_build_group.add_argument('--nodes-file', type=str, help='nodes.dmp from NCBI Taxonomy [Mutually exclusive --retrieve-ncbi]')
    taxsbp_build_group.add_argument('--merged-file', type=str, help='merged.dmp from NCBI Taxonomy [Mutually exclusive --retrieve-ncbi]')
    taxsbp_build_group.add_argument('--taxsbp-path', type=str, default='./', help='TaxSBP script path. Default: ./')

    update_parser = argparse.ArgumentParser(description='Update options', add_help=False)
    default_update_group = update_parser.add_argument_group('general')
    default_update_group.add_argument('-i', '--input-files', required=True, nargs="*", type=str, help='Multi-fasta[.gz] file[s]')
    default_update_group.add_argument('-db', '--db-prefix', required=True, type=str, help='Database prefix')
    default_update_group.add_argument('-o', '--output-db-prefix', type=str, help='Output database prefix (default: overwrite db)')
    default_update_group.add_argument('-t', '--threads', type=int, default=1, help='set the number of subprocesses/threads to use for calculations (default: 1)')
    ganon_update_group = update_parser.add_argument_group('ganon')
    ganon_update_group.add_argument('--ganon-path', type=str, default='./', help='ganon binaries path. Default: ./')
    taxsbp_update_group = update_parser.add_argument_group('taxsbp')  
    taxsbp_update_group.add_argument('--retrieve-ncbi', default=False, action='store_true', help='Retrieve taxdump (--nodes-file, --merged-file) and TaxSBP information (--len-taxid-file) automatically from NCBI')
    taxsbp_update_group.add_argument('--len-taxid-file', type=str, help='Tab-separated file with the sequence ids, sequence length, taxonomic id and optionally the assembly accession in case of assembly level classification is chosen [Mutually exclusive --retrieve-ncbi]')
    taxsbp_update_group.add_argument('--nodes-file', type=str, help='nodes.dmp from NCBI Taxonomy [Mutually exclusive --retrieve-ncbi]')
    taxsbp_update_group.add_argument('--merged-file', type=str, help='merged.dmp from NCBI Taxonomy [Mutually exclusive --retrieve-ncbi]')
    taxsbp_update_group.add_argument('--taxsbp-path', type=str, default='./', help='TaxSBP script path. Default: ./')

    classify_parser = argparse.ArgumentParser(add_help=False)
    classify_group = classify_parser.add_argument_group('Classification options')
    classify_group.add_argument('-db', '--db-prefix', required=True, nargs="*", type=str, help='Database prefix[es]')
    classify_group.add_argument('-c', '--db-hierarchy', type=str, help='Comma separated values to define the order the databases should be used (e.g. 1,1,2,3). Default: 1')
    classify_group.add_argument('-r', '--reads', required=True, nargs="*", type=str, help='Multi-fastq[.gz] file[s] to classify')
    classify_group.add_argument('-t', '--threads', type=int, default=1, help='set the number of subprocesses/threads to use for calculations (default: 1)')
    classify_group.add_argument('-e', '--max-error', type=int, default=1, help='Max. number of errors allowed (default: 3)')
    classify_group.add_argument('-u', '--max-error-unique', type=int, default=None, help='Max. number of errors allowed for unique assignments after filtering. Matches below this error rate will be assigned to parent taxonomic level.')
    classify_group.add_argument('-o', '--output-file', type=str, help='Output file (default: STDOUT)')
    classify_group.add_argument('--skip-lca', default=False, action='store_true', help='Skip LCA step and output multiple matches (max-error-unique will not be applied)')
    classify_group.add_argument('--output-unclassified-file', type=str, help='Output file for unclassified reads')
    classify_group.add_argument('--ganon-path', type=str, default='./', help='ganon binaries path. Default: ./')

    parser = argparse.ArgumentParser(prog='ganon', description='ganon', conflict_handler='resolve')
    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + version)
    subparsers = parser.add_subparsers()
    build = subparsers.add_parser('build', help='Build bloom filter and database', parents=[build_parser])
    build.set_defaults(which='build')
    update = subparsers.add_parser('update', help='Update bloom filter and databse', parents=[update_parser])
    update.set_defaults(which='update')
    classify = subparsers.add_parser('classify', help='Classify reads', parents=[classify_parser])
    classify.set_defaults(which='classify')
    args = parser.parse_args()

    if len(sys.argv[1:])==0: # Print help calling script without parameters
        parser.print_help() 
        return 0
    
    tx_total = time.time()

    # Validations
    if args.which=='build' or args.which=='update':
        if not args.retrieve_ncbi and (not args.len_taxid_file or not args.nodes_file or not args.merged_file):
            sys.stderr.write("--len-taxid-file --nodes-file --merged-file should be provided or --retrieve-ncbi activated")
            return 1
        
        # if -1 set default, if not set value (0 deactivated)
        fragment_length = args.bin_length - args.overlap_length if args.fragment_length==-1 else args.fragment_length
        if args.overlap_length > fragment_length:
            sys.stderr.write("--overlap_length cannot be bigger than --fragment_length")
            return 1
        
        db_prefix = args.db_prefix
        output_folder = os.path.abspath(os.path.dirname(db_prefix)) + "/"
        tmp_output_folder = db_prefix + "_tmp/"
        db_prefix_filter = db_prefix + ".filter"
        db_prefix_nodes = db_prefix + ".nodes"
        db_prefix_map = db_prefix + ".map"
        db_prefix_bins = db_prefix + ".bins"

    elif args.which=='classify':
        if not args.db_hierarchy:
            args.db_hierarchy = ",".join("1"*len(args.db_prefix))

    if args.which=='build':     
        use_assembly=True if args.rank=="assembly" else False
        taxsbp_input_file, taxsbp_nodes_file, taxspb_merged_file = prepare_taxsbp_files(args, tmp_output_folder, use_assembly)

        tx = time.time()
        sys.stderr.write("Running taxonomic clustering (TaxSBP.py)... \n")
        run_taxsbp_cmd = 'python3 {0}TaxSBP.py cluster -f {1} -n {2} -m {3} -l {4} -r {5} {6} {7} {8}'.format(
                            args.taxsbp_path,
                            taxsbp_input_file,
                            taxsbp_nodes_file,
                            taxspb_merged_file,
                            args.bin_length,
                            "assembly" if use_assembly else args.rank,
                            "-z assembly" if use_assembly else "",
                            "-a " + str(fragment_length) if fragment_length else "",
                            "-o " + str(args.overlap_length) if fragment_length else "")
        stdout, stderr, errcode = run(run_taxsbp_cmd, print_stderr=True)
        acc_bin_file = tmp_output_folder + "acc_bin.txt"
        actual_number_of_bins, unique_taxids, max_length_bin = taxsbp_output_files(stdout, acc_bin_file, db_prefix_bins, db_prefix_map, use_assembly, fragment_length)
        sys.stderr.write(str(actual_number_of_bins) + " bins created. ")
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        # define bloom filter size based on given false positive
        if args.min_fp:
            # import re
            # tx = time.time()
            # sys.stderr.write("Spliting files... ")
            # taxsbp_split_files(args.input_files, db_prefix_bins, tmp_output_folder, use_assembly)
            # sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

            # tx = time.time()
            # kmer_counts={}
            # sys.stderr.write("Counting k-mers (kmc)... \n")
            # run_kmc_cmd = "parallel --gnu -j {0} \"mkdir -p {1}{{}}_tmp ; {2}kmc -fm -ci1 -cs1 -b -k{3} -t1 -m{4} {1}{{}}.fna {1}{{}}_tmp/{{}} {1}{{}}_tmp/ | grep 'No. of unique k-mers' | grep -o '[0-9]*' | sed 's/^/{{}}\t/' ; rm -rf {1}{{}}_tmp/\" ::: {5}".format(
            #                 args.threads,
            #                 tmp_output_folder,
            #                 args.kmc_path,
            #                 args.kmer_size,
            #                 args.max_ram,
            #                 " ".join([str(b) for b in list(range(actual_number_of_bins))]))
            # #stdout, stderr, errcode = run(run_kmc_cmd) 
            # stdout, stderr, errcode = run(run_kmc_cmd, output_file=db_prefix + ".kcounts")
            # # todo - do not save to file, parse stdout
            # with open(db_prefix + ".kcounts", "r") as f:
            #     for line in f:
            #         b,kc=line.rstrip().split("\t")
            #         kmer_counts[int(b)] = int(kc)
                    
            # # Calculate the number of bits for the bin with max. number of k-mers
            # max_kmer_count = max(kmer_counts.values())
            # bin_max_kmer_count = max(kmer_counts, key=kmer_counts.get)
            # sys.stderr.write("Max # unique k-mers: " + str(max_kmer_count) + " -> bin " + str(bin_max_kmer_count) + "\n")

            # aproximate number of unique k-mers by just considering they are all unique
            max_kmer_count = max_length_bin-args.kmer_size+1
            sys.stderr.write("Approximate # unique k-mers: " + str(max_kmer_count) +  "\n")

            bin_size_bits = math.ceil(-(1/((1-args.min_fp**(1/float(args.hash_functions)))**(1/float(args.hash_functions*max_kmer_count))-1)))
            # get optimal number of bins to get correct fp rate based on IBF implementation (always next multiple of 64)
            optimal_number_of_bins = math.ceil(actual_number_of_bins/64)*64
            
            sys.stderr.write("Bloom filter calculated size with fp<=" + str(args.min_fp) + ": " + str("{0:.4f}".format((bin_size_bits*optimal_number_of_bins)/8589934592)) + "GB (" + str(bin_size_bits) + " bits/bin * " + str(optimal_number_of_bins) + " optimal bins [" + str(actual_number_of_bins) + " real bins])\n")
            sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Building index (ganon-build)... \n")
        run_ganon_build_cmd = '{0}ganon-build -e {1} {2} {3} -k {4} -n {5} -t {6} -o {7} {8}'.format(
                                        args.ganon_path,
                                        acc_bin_file,
                                        "--bloom-size-bits" if args.min_fp else "-s",
                                        bin_size_bits*optimal_number_of_bins if args.min_fp else args.bloom_size,
                                        args.kmer_size,
                                        args.hash_functions,
                                        args.threads,
                                        db_prefix_filter,
                                        " ".join([file for file in args.input_files]))
        stdout, stderr, errcode = run(run_ganon_build_cmd, print_stderr=True)
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")


        tx = time.time()
        sys.stderr.write("Storing nodes... ")
        info = {'kmer_size':args.kmer_size, 
                'hash_functions': args.hash_functions, 
                'number_of_bins': actual_number_of_bins, 
                'rank': args.rank,
                'bin_length': args.bin_length,
                'filtered_nodes': {}}
        nodes = read_nodes(taxsbp_nodes_file)
        # Filter nodes for used taxids
        for leaf_taxid in unique_taxids:
            t = leaf_taxid
            while t!=0:
                info['filtered_nodes'][t] = nodes[t]
                t = nodes[t]
                if t in info['filtered_nodes']: break # branch already in the dict

        with open(db_prefix_nodes, 'wb') as f: pickle.dump(info, f)

        # Delete temp files
        shutil.rmtree(tmp_output_folder)

        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")
        
    elif args.which=='update':  
        tx = time.time()

        tmp_db_prefix = tmp_output_folder + "tmp"
        tmp_db_prefix_filter = tmp_db_prefix + ".filter"
        tmp_db_prefix_nodes = tmp_db_prefix + ".nodes"
        tmp_db_prefix_map = tmp_db_prefix + ".map"
        tmp_db_prefix_bins = tmp_db_prefix + ".bins"

        kmer_size, hash_functions, number_of_bins, rank, bin_length, filtered_nodes = parse_db_prefix_nodes(db_prefix_nodes)
        use_assembly=True if rank=="assembly" else False

        taxsbp_input_file, taxsbp_nodes_file, taxspb_merged_file = prepare_taxsbp_files(args, tmp_output_folder, use_assembly)

        sys.stderr.write("Running taxonomic clustering (TaxSBP.py)... \n")
        run_taxsbp_cmd = 'python3 {0}TaxSBP.py cluster -f {1} -n {2} -m {3} -l {4} -r {5} {6} -u {7}'.format(
                            args.taxsbp_path,
                            taxsbp_input_file,
                            taxsbp_nodes_file,
                            taxspb_merged_file,
                            bin_length,
                            "assembly" if use_assembly else rank,
                            "-z assembly" if use_assembly else "",
                            db_prefix_bins)
        stdout, stderr, errcode = run(run_taxsbp_cmd, print_stderr=True)
    
        acc_bin_file = tmp_output_folder + "acc_bin.txt"
        updated_bins, unique_taxids, _ = taxsbp_output_files(stdout, acc_bin_file, tmp_db_prefix_bins, tmp_db_prefix_map, use_assembly)
        sys.stderr.write(str(updated_bins) + " bins updated. ")
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Updating index (ganon-update)... \n")
        run_ganon_build_cmd = '{0}ganon-update -e {1} -b {2} -o {3} -t {4} {5}'.format(
                                        args.ganon_path,
                                        acc_bin_file,
                                        db_prefix_filter,
                                        tmp_db_prefix_filter,
                                        args.threads,
                                        " ".join([file for file in args.input_files]))
        stdout, stderr, errcode = run(run_ganon_build_cmd, print_stderr=True)
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Storing nodes... ")
        info = {'kmer_size': kmer_size, 
                'hash_functions': hash_functions, 
                'number_of_bins': number_of_bins, 
                'rank': rank,
                'bin_length': bin_length,
                'filtered_nodes': {}}

        nodes = read_nodes(taxsbp_nodes_file)
        # Filter nodes for used taxids
        for leaf_taxid in unique_taxids:
            t = leaf_taxid
            while t!=0:
                info['filtered_nodes'][t] = nodes[t]
                t = nodes[t]
                if t in info['filtered_nodes']: break # branch already in the dict

        # Merge nodes, duplicates solved by new nodes
        info['filtered_nodes'].update(filtered_nodes)

        with open(tmp_db_prefix_nodes, 'wb') as f: pickle.dump(info, f)
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        # Set new files
        if args.output_db_prefix:
            db_prefix_filter = args.output_db_prefix + ".filter"
            db_prefix_nodes = args.output_db_prefix + ".nodes"
            
            shutil.copyfile(db_prefix_map, args.output_db_prefix + ".map")
            db_prefix_map = args.output_db_prefix + ".map"

            shutil.copyfile(db_prefix_bins, args.output_db_prefix + ".bins")
            db_prefix_bins = args.output_db_prefix + ".bins"


        shutil.move(tmp_db_prefix_filter, db_prefix_filter)
        shutil.move(tmp_db_prefix_nodes, db_prefix_nodes)
        # TODO -> when update adding new bins, also update db_prefix_map file
        # append new bins
        old_bins_file = open(db_prefix_bins, "a")
        with open(tmp_db_prefix_bins, 'r') as file:
            for line in file:
                old_bins_file.write(line)
        old_bins_file.close()

        # Delete temp files
        shutil.rmtree(tmp_output_folder)

    elif args.which=='classify':

        from collections import OrderedDict

        dbs = OrderedDict() 
        sys.stderr.write("Reading nodes and bins... ")
        tx = time.time()

        merged_groupTaxid = {}
        merged_filtered_nodes = {}
        for db_prefix in args.db_prefix:
            kmer_size, hash_functions, number_of_bins, rank, bin_length, filtered_nodes = parse_db_prefix_nodes(db_prefix+".nodes")
            use_assembly=True if rank=="assembly" else False

            merged_groupTaxid.update(parse_db_prefix_bins(db_prefix+".bins", use_assembly))

            dbs[db_prefix] = {'kmer_size': kmer_size, 
                               'hash_functions': hash_functions,
                               'number_of_bins': number_of_bins,
                               'rank': rank,
                               'bin_length': bin_length,
                               'use_assembly': use_assembly
                               }
            merged_filtered_nodes.update(filtered_nodes)
            
        sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        if not args.skip_lca:
            from scripts.LCA import LCA
            tx = time.time()
            sys.stderr.write("Pre-building LCA... ")
            L = LCA(merged_filtered_nodes)
            sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

        sys.stderr.write("Classifying reads (ganon-classify)... \n")
        run_ganon_classify = '{0}ganon-classify {1} {2} -c {3} -e {4} -t {5} {6} {7} {8}'.format(
                                        args.ganon_path,
                                        " ".join(["-b "+db_prefix+".filter" for db_prefix in dbs]),
                                        " ".join(["-g "+db_prefix+".map" for db_prefix in dbs]),
                                        args.db_hierarchy,
                                        args.max_error,
                                        args.threads,
                                        "-u " + str(args.max_error_unique) if args.max_error_unique!=None else "",
                                        "--output-unclassified-file " + args.output_unclassified_file if args.output_unclassified_file else "",
                                        " ".join(args.reads))
        p = subprocess.Popen(shlex.split(run_ganon_classify), shell=False, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        # redirect output
        if args.output_file: sys.stdout = open(args.output_file,'w')
        
        if args.skip_lca:
            done=False
            while not done:
                try:
                    readid, group, kmer_count = read_classified_entry(p)
                    print(readid, group, merged_groupTaxid[group], abs(int(kmer_count)), sep="\t")
                except (ValueError, IndexError): # last line -> empty, no reads classified
                    done=True
        else:
            # read first line
            taxids = set()
            kmer_counts = set()
            try:
                old_readid, old_group, kmer_count = read_classified_entry(p)
                taxids.add(merged_groupTaxid[old_group])
                kmer_counts.add(int(kmer_count))
                count=1
                done=False
            except (ValueError, IndexError): # last line -> empty, no reads classified
                done=True # do not start

            while not done:
                try:
                    readid, group, kmer_count = read_classified_entry(p)
                    taxid = merged_groupTaxid[group]
                except (ValueError, IndexError): # last line -> empty
                    readid=group="" # clear variable to print last entry on this iteration
                    taxid=kmer_count=0
                    done=True # exit on next iteration
                
                # next read matches, print old
                if readid != old_readid: 
                    if count==1: #unique match
                        kc = kmer_counts.pop()
                        tx = taxids.pop()
                        # negative means it was filtered by unique matches in ganon-classify
                        if kc<0: 
                            old_group = "0"
                            if not use_assembly: tx = merged_filtered_nodes[tx] #If classifying at taxonomic level, get parent
                        print(old_readid, old_group, tx, abs(kc), sep="\t")
                    elif len(taxids)==1: # same taxid (but different group) for the matches, manual LCA to save time
                        print(old_readid, "0", taxids.pop(), max(kmer_counts), sep="\t")
                    else: #LCA for multiple matches
                        tmp_lca = L(taxids.pop(),taxids.pop())
                        for i in range(len(taxids)): tmp_lca = L(tmp_lca, taxids.pop())
                        print(old_readid, "0", tmp_lca, max(kmer_counts), sep="\t")
                    kmer_counts.clear()
                    count=0
                
                taxids.add(taxid)
                kmer_counts.add(int(kmer_count))
                count+=1
                old_readid = readid
                old_group = group

        # print stderr
        sys.stderr.write("".join([l for l in p.stderr.readlines()]))

    sys.stderr.write("Total elapsed time: " + str(time.time() - tx_total) + " seconds.\n")
    sys.stdout.close()


def prepare_taxsbp_files(args, tmp_output_folder, use_assembly):
    # Create temporary working directory
    if os.path.exists(tmp_output_folder): shutil.rmtree(tmp_output_folder) # delete if already exists
    os.makedirs(tmp_output_folder)
    
    # Prepare TaxSBP files
    if args.retrieve_ncbi:
        taxsbp_input_file = retrieve_ncbi(tmp_output_folder, args.input_files, args.threads, args.taxsbp_path, use_assembly)
        taxsbp_nodes_file, taxspb_merged_file = get_taxdump(tmp_output_folder)
    else:
        taxsbp_input_file = args.len_taxid_file
        taxsbp_nodes_file = args.nodes_file 
        taxspb_merged_file = args.merged_file

    return taxsbp_input_file, taxsbp_nodes_file, taxspb_merged_file

def read_classified_entry(p):
    if p: return p.stdout.readline().rstrip().split("\t")

def run(cmd, output_file=None, print_stderr=False):
    try:
        errcode=0
        process = subprocess.Popen(shlex.split(cmd), shell=False, universal_newlines=True, stdout=open(output_file, 'w') if output_file else subprocess.PIPE, stderr=subprocess.PIPE)   
        stdout, stderr = process.communicate() # wait for the process to terminate
        errcode = process.returncode
        if errcode!=0: raise Exception()
        if print_stderr: sys.stderr.write(stderr)
        return stdout, stderr, errcode
    #except OSError as e: # The most common exception raised is OSError. This occurs, for example, when trying to execute a non-existent file. Applications should prepare for OSError exceptions.
    #except ValueError as e: #A ValueError will be raised if Popen is called with invalid arguments.
    except Exception as e:
        sys.stderr.write('The following command failed to execute:\n'+cmd)
        sys.stderr.write(str(e)+"\n")
        sys.stderr.write("Errorcode: "+str(errcode)+"\n")
        sys.stderr.write("Error: "+stderr+"\n")
        raise

def fasta_header_generator(file_handler):
    name = ''
    for line in file_handler:
        if line[0] == '>':
            name = line.rstrip()[1:].split(' ', 1)[0]  # omitting the leading > until first space
            yield name

def fasta_read_generator(file_handler):
    seq = []
    name = ''
    for line in file_handler:
        if line[0] == '>':
            sequence = ''.join(seq)
            if name:  # only yield when we already have all data for the first sequence
                yield name, sequence
            name = line.rstrip()[1:]  # omitting the leading >
            seq = []
        else:
            seq += [line]#.rstrip()] # keep line breaks
    sequence = ''.join(seq)
    yield name, sequence  # don't forget the last sequence

def taxsbp_output_files(stdout, acc_bin_file, db_prefix_bins, db_prefix_map, use_assembly, fragment_length):
    bins = open(db_prefix_bins,'w')
    acc_bin = open(acc_bin_file,'w') #.temp for build
    group_bin = open(db_prefix_map,'w') #.map
    unique_taxids = set() # for nodes
    bin_length = defaultdict(int) # calculate max bin length

    for line in stdout.split("\n"):
        if line:
            #acc, length, taxid, [group/rank taxid,] binno
            if use_assembly:
                acc, length, taxid, group, binno = line.split("\t")
            else:
                acc, length, taxid, binno = line.split("\t")
                group = taxid # taxid is the classification group

            # if sequences are fragmentes
            if fragment_length: 
                acc, coord = acc.split("/")
                frag_start, frag_end = coord.split(":")
            else:
                frag_start = 1
                frag_end = length

            bin_length[binno]=+int(length)
            unique_taxids.add(int(taxid))
            print(line, file=bins) 
            print(acc, frag_start, frag_end, binno, sep=" ", file=acc_bin)
            print(group, binno, sep=" ", file=group_bin)
    bins.close()
    acc_bin.close()
    group_bin.close()
    return len(bin_length), unique_taxids, max(bin_length.values())

# def taxsbp_split_files(files, taxsbp_bins_file, output_folder, use_assembly):
#     bins = {}
#     with open(taxsbp_bins_file, 'r') as file:
#         for line in file:
#             fields = line.rstrip().split("\t")
#             # bins[seqid] = binid
#             bins[fields[0]] = fields[3] if not use_assembly else fields[4]

#     for file in files:
#         handler = gzip.open(file, 'rt') if file.endswith(".gz") else open(file,'r')
#         for header, sequence in fasta_read_generator(handler):
#             if header.split(' ', 1)[0] in bins:
#                 outfilename = output_folder + bins[header.split(' ', 1)[0]] + ".fna"
#                 outfile = open(outfilename, 'a')
#                 outfile.write('>' + header + '\n' + sequence)
#                 outfile.close()

def get_taxdump(tmp_output_folder):
    tx = time.time()
    sys.stderr.write("Downloading taxdump... ")
    taxdump_file = tmp_output_folder+'taxdump.tar.gz'
    run_wget_taxdump_cmd = 'wget -qO {0} ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz'.format(taxdump_file)
    stdout, stderr, errcode = run(run_wget_taxdump_cmd, print_stderr=True)
    unpack_taxdump_cmd = 'tar xf {0} -C "{1}" nodes.dmp merged.dmp'.format(taxdump_file, tmp_output_folder)
    stdout, stderr, errcode = run(unpack_taxdump_cmd, print_stderr=True)
    sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")
    return tmp_output_folder+'nodes.dmp', tmp_output_folder+'merged.dmp'

def retrieve_ncbi(tmp_output_folder, files, threads, taxsbp_path, use_assembly):
    tx = time.time()
    sys.stderr.write("Extracting accessions... ")
    ids = []
    for file in files:
        handler = gzip.open(file, 'rt') if file.endswith(".gz") else open(file,'r')
        ids.extend([header for header in fasta_header_generator(handler)])
    f=open(tmp_output_folder + 'accessions.txt','w')
    f.write('\n'.join(ids))
    f.close()
    sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")

    tx = time.time()
    sys.stderr.write("Retrieving data from NCBI... ")
    taxsbp_input_file = tmp_output_folder + 'acc_len_taxid.txt'
    run_get_len_taxid_cmd = 'parallel --gnu -j {0} -a {1} {2}{3} {{}}'.format(
                            threads,
                            tmp_output_folder + 'accessions.txt',
                            taxsbp_path+"/scripts/",
                            "get_len_taxid_assembly.sh" if use_assembly else "get_len_taxid.sh")
    _, stderr, errcode = run(run_get_len_taxid_cmd, output_file=taxsbp_input_file, print_stderr=True)
    sys.stderr.write("Done. Elapsed time: " + str(time.time() - tx) + " seconds.\n")
    return taxsbp_input_file

def read_nodes(nodes_file):
    # READ nodes -> fields (1:TAXID 2:PARENT_TAXID)
    nodes = {}
    with open(nodes_file,'r') as fnodes:
        for line in fnodes:
            taxid, parent_taxid, _, _ = line.split('\t|\t',3)
            nodes[int(taxid)] = int(parent_taxid)
    nodes[1] = 0 
    return nodes

def parse_db_prefix_bins(file, use_assembly):
    groupTaxid = {}
    with open(file,'r') as bins:
        for line in bins:
            if use_assembly:
                acc, length, taxid, group, binno = line.split("\t")
            else:
                acc, length, taxid, binno = line.split("\t")
                group = taxid # taxid is the classification group
            groupTaxid[group] = int(taxid)
    return groupTaxid

def parse_db_prefix_nodes(file):
    info = pickle.load(open(file, "rb"))
    kmer_size = info['kmer_size']
    hash_functions = info['hash_functions']
    number_of_bins = info['number_of_bins']
    rank = info['rank']
    bin_length = info['bin_length']
    filtered_nodes = info['filtered_nodes']
    return kmer_size, hash_functions, number_of_bins, rank, bin_length, filtered_nodes

if __name__ == '__main__':
    main()
