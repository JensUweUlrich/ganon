#!/usr/bin/python3

import argparse, os, sys, subprocess, io, time, shlex, shutil, gzip, pickle, math, re
from collections import defaultdict

def main():
    version = '1.0'

    ####################################################################################################

    build_parser = argparse.ArgumentParser(description='Build options', add_help=False)
    
    # Required
    build_group_required = build_parser.add_argument_group('required arguments')
    build_group_required.add_argument('-d', '--db-prefix',      required=True, type=str,                    metavar='db_prefix',        help='Database output prefix (.filter, .nodes, .bins. .map will be created)')
    build_group_required.add_argument('-i', '--input-files',    required=True, type=str, nargs="*",         metavar='refs.fasta[.gz]',  help='Multi-fasta[.gz] file[s]')

    # Defaults
    build_group_optional = build_parser.add_argument_group('optional arguments')
    build_group_optional.add_argument('-k', '--kmer-size',                      type=int,   default=19,     metavar='', help='The k-mer size for the bloom filter [14..32]. Default: 19')
    build_group_optional.add_argument('-n', '--hash-functions',                 type=int,   default=3,      metavar='', help='The number of hash functions to use for the bloom filter [2..5]. Default: 3')
    build_group_optional.add_argument('-f', '--max-fp',                         type=float, default=0.05,   metavar='', help='Max. false positive rate for k-mer classification. Filter size will be defined base on this value by the estimated number of unique k-mers [Mutually exclusive --bloom-size]. Default: 0.05')
    build_group_optional.add_argument('-s', '--bloom-size',                     type=int,                   metavar='', help='Fixed size for filter in Megabytes (MB) [Mutually exclusive --min-fp] ')
    build_group_optional.add_argument('-t', '--threads',                        type=int,   default=2,      metavar='', help='set the number of subprocesses/threads to use for calculations. Default: 2')
    build_group_optional.add_argument('-r', '--rank',                           type=str,   default='taxid', metavar='', help='Lowest taxonomic rank for classification [assembly,taxid,species,genus,...]. Default: taxid')
    build_group_optional.add_argument('-l', '--bin-length',                     type=int,   default=6000000, metavar='', help='Maximum length (in bp) for each bin. Default: 6000000')
    build_group_optional.add_argument('-a', '--fragment-length',                type=int,   default=-1,      metavar='', help='Fragment length (in bp). Set to 0 to not fragment sequences. (default: bin-length - overlap-length)')
    build_group_optional.add_argument('-o', '--overlap-length',                 type=int,   default=500,     metavar='', help='Fragment overlap length (in bp). Should be bigger than the read length used for classificaiton. Default: 500')
    build_group_optional.add_argument('--len-taxid-file',                       type=str,                   metavar='', help='Tab-separated file with the sequence ids, sequence length, taxonomic id and optionally the assembly accession in case of assembly level classification is chosen')
    build_group_optional.add_argument('--nodes-file',                           type=str,                   metavar='', help='nodes.dmp from NCBI Taxonomy')
    build_group_optional.add_argument('--merged-file',                          type=str,                   metavar='', help='merged.dmp from NCBI Taxonomy')
    build_group_optional.add_argument('--names-file',                           type=str,                   metavar='', help='names.dmp from NCBI Taxonomy')
    # Extra
    build_group_optional.add_argument('--verbose',                                          default=False, action='store_true', help='Verbose mode for ganon')
    build_group_optional.add_argument('--ganon-path', type=str, default="./", help=argparse.SUPPRESS)
    build_group_optional.add_argument('--taxsbp-path', type=str, default="./", help=argparse.SUPPRESS)
    build_group_optional.add_argument('--n-refs', type=int, help=argparse.SUPPRESS)
    build_group_optional.add_argument('--n-batches', type=int, help=argparse.SUPPRESS)

    ####################################################################################################

    update_parser = argparse.ArgumentParser(description='Update options', add_help=False)

    # Required
    update_group_required = update_parser.add_argument_group('required arguments')
    update_group_required.add_argument('-d', '--db-prefix',         required=True,  type=str,               metavar='db_prefix',        help='Database prefix')
    update_group_required.add_argument('-i', '--input-files',       required=True,  type=str, nargs="*",    metavar='refs.fasta[.gz]',  help='Multi-fasta[.gz] file[s]')
    
    # Defaults
    update_group_optional = update_parser.add_argument_group('optional arguments')
    update_group_optional.add_argument('-o', '--output-db-prefix',                  type=str,               metavar='', help='Alternative output database prefix. Default: overwrite current --db-prefix')
    #update_group_optional.add_argument('-c', '--update-complete',                             default=False, action='store_true', help='Update complete bins, removing sequences. Input file should be complete, not only new sequences.')
    update_group_optional.add_argument('-t', '--threads',                           type=int, default=2,    metavar='', help='set the number of subprocesses/threads to use for calculations. Default: 2')
    update_group_optional.add_argument('--len-taxid-file',                          type=str,               metavar='', help='Tab-separated file with the sequence ids, sequence length, taxonomic id and optionally the assembly accession in case of assembly level classification is chosen')
    update_group_optional.add_argument('--nodes-file',                              type=str,               metavar='', help='nodes.dmp from NCBI Taxonomy')
    update_group_optional.add_argument('--merged-file',                             type=str,               metavar='', help='merged.dmp from NCBI Taxonomy')
    update_group_optional.add_argument('--names-file',                              type=str,               metavar='', help='names.dmp from NCBI Taxonomy')
    # Extra
    update_group_optional.add_argument('--verbose',                                           default=False, action='store_true', help='Verbose mode for ganon')
    update_group_optional.add_argument('--ganon-path', type=str, default="./", help=argparse.SUPPRESS)
    update_group_optional.add_argument('--taxsbp-path', type=str, default="./", help=argparse.SUPPRESS)
    update_group_optional.add_argument('--n-refs', type=int, help=argparse.SUPPRESS)
    update_group_optional.add_argument('--n-batches', type=int, help=argparse.SUPPRESS)

    ####################################################################################################

    classify_parser = argparse.ArgumentParser(description='Classification options', add_help=False)
    
    # Required
    classify_group_required = classify_parser.add_argument_group('required arguments')
    classify_group_required.add_argument('-d', '--db-prefix',    required=True, type=str,              nargs="*", metavar='db_prefix', help='Database prefix[es]')
    classify_group_required.add_argument('-r', '--reads',        required=True, type=str,              nargs="*", metavar='reads.fq[.gz]', help='Multi-fastq[.gz] file[s] to classify')

    # Defaults
    classify_group_optional = classify_parser.add_argument_group('optional arguments')
    classify_group_optional.add_argument('-c', '--db-hierarchy',                type=str, default=["1"], nargs="*", metavar='int', help='Hierachy definition, one for each database input. Can also be string, but input will be sorted (e.g. 1 1 2 3). Default: 1')
    classify_group_optional.add_argument('-e', '--max-error',                   type=int, default=[3],   nargs="*", metavar='int', help='Max. number of errors allowed. Single value or one per database (e.g. 3 3 4 0). Default: 3')
    classify_group_optional.add_argument('-u', '--max-error-unique',            type=int, default=[-1],  nargs="*", metavar='int', help='Max. number of errors allowed for unique assignments after filtering. Matches below this error rate will not be discarded, but assigned to parent taxonomic level. Single value or one per hierachy (e.g. 0 1 2). -1 to disable. Default: -1')
    classify_group_optional.add_argument('-f', '--offset',                      type=int, default=1,              metavar='', help='Number of k-mers to skip during clasification. Can speed up analysis but may reduce recall. (e.g. 1 = all k-mers, 3 = every 3rd k-mer). Function must be enabled on compilation time with -DGANON_OFFSET=ON. Default: 1')
    classify_group_optional.add_argument('-o', '--output-file-prefix',          type=str, default="",             metavar='', help='Output file name prefix: .out for complete results / .lca for LCA results / .rep for report. Empty to print to STDOUT (only with lca). Default: ""')
    classify_group_optional.add_argument('-n', '--output-unclassified-file',    type=str, default="",             metavar='', help='Output file for unclassified reads headers. Empty to not output. Default: ""')
    classify_group_optional.add_argument('-s', '--split-output-file-hierarchy', default=False, action='store_true',               help='Split output in multiple files by hierarchy. Appends "_hierachy" to the --output-file definiton.')
    classify_group_optional.add_argument('-l', '--skip-lca',                    default=False, action='store_true',               help='Skip LCA step and output multiple matches. --max-error-unique will not be applied')
    classify_group_optional.add_argument('-p', '--skip-reports',                default=False, action='store_true',               help='Skip reports')
    classify_group_optional.add_argument('-k', '--ranks',                       type=str, default=[],   nargs="*",                help='Ranks for the final report. "all" for all indentified ranks. empty for default ranks: superkingdom phylum class order family genus species species+ assembly')
    classify_group_optional.add_argument('-t', '--threads',                     type=int, default=3,              metavar='', help='Number of subprocesses/threads. Default: 3)')
    # Extra
    classify_group_optional.add_argument('--verbose',                           default=False, action='store_true',  help='Output in verbose mode for ganon-classify')
    classify_group_optional.add_argument('--ganon-path', type=str, default="./", help=argparse.SUPPRESS)
    classify_group_optional.add_argument('--n-reads', type=int, help=argparse.SUPPRESS)
    classify_group_optional.add_argument('--n-batches', type=int, help=argparse.SUPPRESS)

    ####################################################################################################

    parser = argparse.ArgumentParser(prog='ganon', description='ganon', conflict_handler='resolve')
    parser.add_argument('-v', '--version', action='version', version='%(prog)s ' + version, help="Show program's version number and exit.")
    subparsers = parser.add_subparsers()
    
    build = subparsers.add_parser('build', help='Build ganon database', parents=[build_parser])
    build.set_defaults(which='build')

    update = subparsers.add_parser('update', help='Update ganon database', parents=[update_parser])
    update.set_defaults(which='update')

    classify = subparsers.add_parser('classify', help='Classify reads', parents=[classify_parser])
    classify.set_defaults(which='classify')

    args = parser.parse_args()

    if len(sys.argv[1:])==0: # Print help calling script without parameters
        parser.print_help() 
        return 0
    
    tx_total = time.time()

    # Validations
    if not os.path.isfile(args.ganon_path + "/ganon-classify") or not os.path.isfile(args.ganon_path + "/ganon-build"):
        sys.stderr.write("ganon binaries (ganon-classify and ganon-build) were not found. Please inform the path of the binaries with --ganon-path\n")
        return 1

    if args.which=='build' or args.which=='update':
        db_prefix = args.db_prefix
        output_folder = os.path.abspath(os.path.dirname(db_prefix)) + "/"
        tmp_output_folder = db_prefix + "_tmp/"
        db_prefix_filter = db_prefix + ".filter"
        db_prefix_nodes = db_prefix + ".nodes"
        db_prefix_map = db_prefix + ".map"
        db_prefix_bins = db_prefix + ".bins"
        
        if not os.path.isfile(args.taxsbp_path + "/TaxSBP.py") or not os.path.isfile(args.taxsbp_path + "/scripts/get_len_taxid.sh"):
            sys.stderr.write("TaxSBP files (TaxSBP.py and scripts/get_len_taxid.sh) were not found. Please inform the path of the files with --taxsbp-path\n")
            return 1

    if args.which=='build':
        # if -1 set default, if not set value (0 deactivated)
        if args.fragment_length==-1:
            fragment_length = args.bin_length - args.overlap_length
        elif args.fragment_length==0:
            fragment_length = 0
        else:
            fragment_length = args.fragment_length - args.overlap_length 

        if fragment_length and args.overlap_length > fragment_length:
            sys.stderr.write("--overlap_length cannot be bigger than --fragment_length\n")
            return 1

    if args.which=='classify':
        if args.skip_lca and not args.output_file_prefix:
            sys.stderr.write("--output-file-prefix is mandatory without LCA\n")
            return 1

    if args.which=='build':     
        use_assembly=True if args.rank=="assembly" else False
        taxsbp_input_file, ncbi_nodes_file, ncbi_merged_file, ncbi_names_file = prepare_taxsbp_files(args, tmp_output_folder, use_assembly)

        tx = time.time()
        sys.stderr.write("Running taxonomic clustering (TaxSBP.py)... \n")
        run_taxsbp_cmd = 'python3 {0}TaxSBP.py cluster -f {1} -n {2} {3} -l {4} -r {5} {6} {7} {8}'.format(
                            args.taxsbp_path,
                            taxsbp_input_file,
                            ncbi_nodes_file,
                            "-m " + ncbi_merged_file if ncbi_merged_file else "",
                            args.bin_length,
                            "assembly" if use_assembly else args.rank,
                            "-z assembly" if use_assembly else "",
                            "-a " + str(fragment_length) if fragment_length else "",
                            "-o " + str(args.overlap_length) if fragment_length else "")
        stdout, stderr, errcode = run(run_taxsbp_cmd, print_stderr=True)
        acc_bin_file = tmp_output_folder + "acc_bin.txt"
        actual_number_of_bins, unique_taxids, max_length_bin, group_taxid = taxsbp_output_files(stdout, acc_bin_file, db_prefix_bins, db_prefix_map, use_assembly, fragment_length)
        sys.stderr.write(str(actual_number_of_bins) + " bins created. ")
        sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")

        # aproximate number of unique k-mers by just considering that they are all unique
        max_kmer_count = max_length_bin-args.kmer_size+1
        sys.stderr.write("Approximate (upper-bound) # unique k-mers: " + str(max_kmer_count) + "\n")
        # get optimal number of bins to get correct fp rate based on IBF implementation (always next multiple of 64)
        optimal_number_of_bins = math.ceil(actual_number_of_bins/64)*64

        # define bloom filter size based on given false positive
        MBinBits = 8388608
        if not args.bloom_size:
            bin_size_bits = math.ceil(-(1/((1-args.max_fp**(1/float(args.hash_functions)))**(1/float(args.hash_functions*max_kmer_count))-1)))   
            sys.stderr.write("Bloom filter calculated size with fp<=" + str(args.max_fp) + ": " + str("{0:.4f}".format((bin_size_bits*optimal_number_of_bins)/MBinBits)) + "MB (" + str(bin_size_bits) + " bits/bin * " + str(optimal_number_of_bins) + " optimal bins [" + str(actual_number_of_bins) + " real bins])\n")
        else:
            bin_size_bits = math.ceil((args.bloom_size * MBinBits)/optimal_number_of_bins);
            estimated_max_fp = (1-((1-(1/float(bin_size_bits)))**(args.hash_functions*max_kmer_count)))**args.hash_functions
            sys.stderr.write("Bloom filter calculated max. fp with size=" + str(args.bloom_size) + "MB: " + str("{0:.4f}".format(estimated_max_fp) + " ("  + str(optimal_number_of_bins) + " optimal bins [" + str(actual_number_of_bins) + " real bins])\n"))

        tx = time.time()
        sys.stderr.write("Building index (ganon-build)... \n")
        run_ganon_build_cmd = '{0}ganon-build -e {1} {2} {3} -k {4} -n {5} -t {6} -o {7} {8} {9} {10} {11}'.format(
                                        args.ganon_path,
                                        acc_bin_file,
                                        "--filter-size-bits" if args.max_fp else "--filter-size",
                                        bin_size_bits*optimal_number_of_bins if args.max_fp else args.bloom_size,
                                        args.kmer_size,
                                        args.hash_functions,
                                        args.threads,
                                        db_prefix_filter,
                                        "--verbose" if args.verbose else "",
                                        "--n-refs " + str(args.n_refs) if args.n_refs is not None else "",
                                        "--n-batches " + str(args.n_batches) if args.n_batches is not None else "",
                                        " ".join([file for file in args.input_files]))
        stdout, stderr, errcode = run(run_ganon_build_cmd, print_stderr=True)
        sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Building database files... ")

        info = {'kmer_size':args.kmer_size, 
                'hash_functions': args.hash_functions, 
                'number_of_bins': actual_number_of_bins, 
                'rank': args.rank,
                'bin_length': args.bin_length,
                'fragment_length': fragment_length,
                'overlap_length': args.overlap_length,
                'filtered_nodes': build_filtered_nodes(unique_taxids, group_taxid, ncbi_nodes_file, ncbi_names_file, use_assembly)}
        
        with open(db_prefix_nodes, 'wb') as f: pickle.dump(info, f)

        # Delete temp files
        shutil.rmtree(tmp_output_folder)

        sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")
        
        
    elif args.which=='update':  
        tx = time.time()

        tmp_db_prefix = tmp_output_folder + "tmp"
        tmp_db_prefix_filter = tmp_db_prefix + ".filter"
        tmp_db_prefix_nodes = tmp_db_prefix + ".nodes"
        tmp_db_prefix_map = tmp_db_prefix + ".map"
        tmp_db_prefix_bins = tmp_db_prefix + ".bins"

        kmer_size, hash_functions, number_of_bins, rank, bin_length, fragment_length, overlap_length, filtered_nodes = parse_db_prefix_nodes(db_prefix_nodes)
        use_assembly=True if rank=="assembly" else False

        taxsbp_input_file, ncbi_nodes_file, ncbi_merged_file, ncbi_names_file = prepare_taxsbp_files(args, tmp_output_folder, use_assembly)

        sys.stderr.write("Running taxonomic clustering (TaxSBP.py)... \n")
        run_taxsbp_cmd = 'python3 {0}TaxSBP.py cluster -u {1} -f {2} -n {3} {4} -l {5} -r {6} {7} {8} {9}'.format(
                            args.taxsbp_path,
                            db_prefix_bins,
                            taxsbp_input_file,
                            ncbi_nodes_file,
                            "-m " + ncbi_merged_file if ncbi_merged_file else "",
                            bin_length,
                            "assembly" if use_assembly else rank,
                            "-z assembly" if use_assembly else "",
                            "-a " + str(fragment_length) if fragment_length else "",
                            "-o " + str(overlap_length) if fragment_length else "")
        stdout, stderr, errcode = run(run_taxsbp_cmd, print_stderr=True)
    
        acc_bin_file = tmp_output_folder + "acc_bin.txt"
        updated_bins, unique_taxids, _, group_taxid = taxsbp_output_files(stdout, acc_bin_file, tmp_db_prefix_bins, tmp_db_prefix_map, use_assembly, fragment_length)
        sys.stderr.write(str(updated_bins) + " bins updated. ")
        sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Updating index (ganon-build)... \n")
        run_ganon_build_cmd = '{0}ganon-build -u {1} -e {2} -o {3} -t {4} {5} {6} {7} {8}'.format(
                                        args.ganon_path,
                                        db_prefix_filter,
                                        acc_bin_file,
                                        tmp_db_prefix_filter,
                                        args.threads,
                                        "--verbose" if args.verbose else "",
                                        "--n-refs " + str(args.n_refs) if args.n_refs is not None else "",
                                        "--n-batches " + str(args.n_batches) if args.n_batches is not None else "",
                                        " ".join([file for file in args.input_files]))
        stdout, stderr, errcode = run(run_ganon_build_cmd, print_stderr=True)
        sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")

        tx = time.time()
        sys.stderr.write("Updating database files ... ")

        # generate new nodes and names
        new_filtered_nodes = build_filtered_nodes(unique_taxids, group_taxid, ncbi_nodes_file, ncbi_names_file, use_assembly)

        # Merge nodes, duplicates solved by new nodes
        filtered_nodes.update(new_filtered_nodes)

        info = {'kmer_size': kmer_size, 
                'hash_functions': hash_functions, 
                'number_of_bins': number_of_bins, 
                'rank': rank,
                'bin_length': bin_length,
                'fragment_length': fragment_length,
                'overlap_length': overlap_length,
                'filtered_nodes': filtered_nodes}
        with open(tmp_db_prefix_nodes, 'wb') as f: pickle.dump(info, f)

        # Set new files
        if args.output_db_prefix:
            db_prefix_filter = args.output_db_prefix + ".filter"
            db_prefix_nodes = args.output_db_prefix + ".nodes"
            
            shutil.copyfile(db_prefix_map, args.output_db_prefix + ".map")
            db_prefix_map = args.output_db_prefix + ".map"

            shutil.copyfile(db_prefix_bins, args.output_db_prefix + ".bins")
            db_prefix_bins = args.output_db_prefix + ".bins"

        # move temp db to chosen output
        shutil.move(tmp_db_prefix_filter, db_prefix_filter)
        shutil.move(tmp_db_prefix_nodes, db_prefix_nodes)
        
        # append new map
        old_map_file = open(db_prefix_map, "a")
        with open(tmp_db_prefix_map, 'r') as file:
            for line in file:
                old_map_file.write(line) # TODO -> remove duplicated lines
        old_map_file.close()

        # append new bins
        old_bins_file = open(db_prefix_bins, "a")
        with open(tmp_db_prefix_bins, 'r') as file:
            for line in file:
                old_bins_file.write(line)
        old_bins_file.close()

        # Delete temp files
        shutil.rmtree(tmp_output_folder)

        sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")

    elif args.which=='classify':

        if not args.output_file_prefix:
            ganon_classify_output_file = "ganon-classify-tmp.out" 
            args.skip_reports=True #output only stdout, no reports
        else:
            ganon_classify_output_file = args.output_file_prefix+".out"

        tx = time.time()
        sys.stderr.write("Classifying reads (ganon-classify)... \n")
        run_ganon_classify = '{0}ganon-classify {1} {2} -c {3} -e {4} -u {5} -t {6} -f {7} -o {8} {9} {10} {11} {12} {13} {14}'.format(
                                        args.ganon_path,
                                        " ".join(["-b "+db_prefix+".filter" for db_prefix in args.db_prefix]),
                                        " ".join(["-g "+db_prefix+".map" for db_prefix in args.db_prefix]),
                                        ",".join([str(h) for h in args.db_hierarchy]),
                                        ",".join([str(me) for me in args.max_error]),
                                        ",".join([str(meu) for meu in args.max_error_unique]),
                                        args.threads,
                                        args.offset,
                                        ganon_classify_output_file,
                                        "-s" if args.split_output_file_hierarchy else "",
                                        "-n " + args.output_unclassified_file if args.output_unclassified_file else "",
                                        "--verbose" if args.verbose else "",
                                        "--n-reads " + str(args.n_reads) if args.n_reads is not None else "",
                                        "--n-batches " + str(args.n_batches) if args.n_batches is not None else "",
                                        " ".join(args.reads))
        stdout, stderr, errcode = run(run_ganon_classify, print_stderr=True)
        sys.stderr.write("\nDone. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")

        if not args.skip_lca:
            from scripts.LCA import LCA
            sys.stderr.write("Generating LCA and reports... ")
            tx = time.time()

            if not args.skip_reports:
                # get reads classified and unclassified from strerr
                re_out = re.search(r"\d+\ssequences classified", stderr)
                seq_cla = int(re_out.group().split(" ")[0]) if re_out is not None else 0
                re_out = re.search(r"\d+\ssequences unclassified", stderr)
                seq_unc = int(re_out.group().split(" ")[0]) if re_out is not None else 0
                total_reads = seq_cla+seq_unc

            reports = {}
            merged_filtered_nodes = {}

            # build hierarchy structure for output lca files
            output_hierarchy = defaultdict(list)
            if len(args.db_hierarchy) > 1 and args.split_output_file_hierarchy:
                for dbid,dbp in enumerate(args.db_prefix):
                    output_hierarchy[args.db_hierarchy[dbid]].append(dbp)
            else:
                output_hierarchy[None] = args.db_prefix # no slit or no hierarchy, output together
 
            for hierarchy_name, db_prefixes in output_hierarchy.items():

                # check if prefix for multiple files is necessary
                h_prefix = "_"+hierarchy_name if hierarchy_name is not None else ""
                
                # Merge group-taxid information and nodes from multiple databases
                merged_filtered_nodes[hierarchy_name] = {}
                for db_prefix in db_prefixes:
                    _, _, _, rank, _, _, _, filtered_nodes = parse_db_prefix_nodes(db_prefix+".nodes")
                    use_assembly=True if rank=="assembly" else False # if one of them uses assembly should be True
                    merged_filtered_nodes[hierarchy_name].update(filtered_nodes)

                #### to remove - compability with older dbs ###
                if 1 in merged_filtered_nodes[hierarchy_name]:
                    new_filtered_nodes = {}
                    for t,p in merged_filtered_nodes[hierarchy_name].items():
                        new_filtered_nodes[str(t)] = (str(p),"","")
                    merged_filtered_nodes[hierarchy_name] = new_filtered_nodes
                    if not args.skip_reports:
                        args.skip_reports=True
                        sys.stderr.write("\n\n ------- \n It was not possible to generate reports because you are using an outdate database version \n Please re-create your indices with this version to be able to generate reports \n ------- \n\n")
                #### to remove - compability with older dbs ###

                # pre build LCA with used nodes
                L = LCA({tx:parent for tx,(parent,_,_) in merged_filtered_nodes[hierarchy_name].items()})

                # redirect output for lca
                if args.output_file_prefix: sys.stdout = open(args.output_file_prefix+".lca"+h_prefix,'w')

                rep = defaultdict(lambda: {'count': 0, 'assignments': 0, 'unique': 0, 'sum_kmer_count': 0})

                with open(ganon_classify_output_file+h_prefix) as file:
                    try: # read first entry
                        old_readid, cl, kc = file.readline().rstrip().split("\t")
                        assignments = set([cl])
                        max_kmer_count = int(kc)
                        sum_kmer_count = max_kmer_count
                        done = False
                    except (ValueError, IndexError): # last line -> empty, no reads classified
                        done=True # do not start
                    
                    while not done:
                        try:
                            readid, cl, kc = file.readline().rstrip().split("\t")
                        except (ValueError, IndexError): # last line -> empty
                            readid="" # clear variable to print last entry on this iteration
                            done=True # exit on next iteration
                            
                        # next read matches, print old
                        if readid != old_readid: 
                            len_assignments = len(assignments)
                            taxid_lca = get_lca_read(assignments, max_kmer_count, merged_filtered_nodes[hierarchy_name], L, use_assembly)
                            print(old_readid, taxid_lca, max_kmer_count, sep="\t")
                            if not args.skip_reports:
                                rep[taxid_lca]['count'] += 1
                                rep[taxid_lca]['assignments'] += len_assignments
                                if len_assignments==1 and max_kmer_count>0: rep[taxid_lca]['unique'] += 1 # only count as unique if was not negative (meaning it didn't pass unique error filter)
                                rep[taxid_lca]['sum_kmer_count'] += sum_kmer_count

                            #assignments.clear() -> should be empty here
                            max_kmer_count=0
                            sum_kmer_count=0
                    
                        if not done: #if not last line
                            assignments.add(cl)
                            kc = int(kc)
                            # account for unique filtering with abs but keep it negative to retain information on output
                            max_kmer_count = kc if abs(kc) > abs(max_kmer_count) else max_kmer_count 
                            sum_kmer_count+=abs(kc)
                            old_readid = readid
                    
                # Rm file if tmp
                if not args.output_file_prefix: 
                	os.remove(ganon_classify_output_file+h_prefix) 
               	else: # close open file if not
               	    sys.stdout.close()
                    sys.stdout = sys.__stdout__ #return to stdout

                # print single report file
                if not args.skip_reports: 
                    with open(args.output_file_prefix+".rep"+h_prefix, 'w') as rfile:
                        rfile.write("unclassified" +"\t"+ str(seq_unc) +"\t"+ str("%.5f" % ((seq_unc/total_reads)*100)) +"\t"+ "0" +"\t"+ "0" +"\t"+ "0" +"\t"+ "-" +"\t"+ "-" + "\n")
                        for assignment in sorted(rep, key=lambda k: rep[k]['count'], reverse=True):
                            rfile.write(assignment +"\t"+ str(rep[assignment]['count']) +"\t"+ str("%.5f" % ((rep[assignment]['count']/total_reads)*100)) +"\t"+ str(rep[assignment]['assignments']) +"\t"+ str(rep[assignment]['unique']) +"\t"+ str(rep[assignment]['sum_kmer_count']) +"\t"+ merged_filtered_nodes[hierarchy_name][assignment][2] +"\t"+ merged_filtered_nodes[hierarchy_name][assignment][1] + "\n")
                reports[hierarchy_name] = rep

            if not args.skip_reports:
                final_report_file = args.output_file_prefix+".tre"
                print_final_report(reports, merged_filtered_nodes, seq_unc, total_reads, final_report_file, args.ranks)

            sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")
    sys.stderr.write("Total elapsed time: " + str("%.2f" % (time.time() - tx_total)) + " seconds.\n")
   
def get_lca_read(assignments, max_kmer_count, merged_filtered_nodes, L, use_assembly):
    if len(assignments)==1: # unique match or same taxid (but one or more assemblies)  
        if max_kmer_count<0: # unique matches in ganon-classify, get leaf taxid (assembly) or parent taxid
            return merged_filtered_nodes[assignments.pop()][0]
        else:
            return assignments.pop()
    else:
        if use_assembly: # get taxids from assembly (could be done on the LCA, but is done here for speed-up)
            assignments = set(map(lambda x: merged_filtered_nodes[x][0], assignments))  # Recover taxids from assignments (assembly)
            if len(assignments)==1: # If all assignments are on the same taxid, no need to lca and return it
                return assignments.pop()

        taxid_lca = L(assignments.pop(),assignments.pop())
        for i in range(len(assignments)): 
            taxid_lca = L(taxid_lca, assignments.pop())
            if taxid_lca == 1: 
                assignments.clear()
                break #if lca is already root node, no point in continue

        return taxid_lca

def prepare_taxsbp_files(args, tmp_output_folder, use_assembly):
    # Create temporary working directory
    if os.path.exists(tmp_output_folder): shutil.rmtree(tmp_output_folder) # delete if already exists
    os.makedirs(tmp_output_folder)
    
    # Prepare TaxSBP files
    if not args.len_taxid_file:
        taxsbp_input_file = retrieve_ncbi(tmp_output_folder, args.input_files, args.threads, args.taxsbp_path, use_assembly)
    else:
        taxsbp_input_file = args.len_taxid_file

    if not args.nodes_file:
        ncbi_nodes_file, ncbi_merged_file, ncbi_names_file = get_taxdump(tmp_output_folder)
    else:
        ncbi_nodes_file = args.nodes_file 
        ncbi_merged_file = args.merged_file
        ncbi_names_file = args.names_file

    return taxsbp_input_file, ncbi_nodes_file, ncbi_merged_file, ncbi_names_file

def run(cmd, output_file=None, print_stderr=False):
    try:
        errcode=0
        process = subprocess.Popen(shlex.split(cmd), shell=False, universal_newlines=True, stdout=open(output_file, 'w') if output_file else subprocess.PIPE, stderr=subprocess.PIPE)   
        stdout, stderr = process.communicate() # wait for the process to terminate
        errcode = process.returncode
        if errcode!=0: raise Exception()
        if print_stderr: sys.stderr.write(stderr)
        return stdout, stderr, errcode
    #except OSError as e: # The most common exception raised is OSError. This occurs, for example, when trying to execute a non-existent file. Applications should prepare for OSError exceptions.
    #except ValueError as e: #A ValueError will be raised if Popen is called with invalid arguments.
    except Exception as e:
        sys.stderr.write('The following command failed to execute:\n'+cmd)
        sys.stderr.write(str(e)+"\n")
        sys.stderr.write("Errorcode: "+str(errcode)+"\n")
        sys.stderr.write("Error: "+stderr+"\n")
        raise

def taxsbp_output_files(stdout, acc_bin_file, db_prefix_bins, db_prefix_map, use_assembly, fragment_length):
    bins = open(db_prefix_bins,'w')
    acc_bin = open(acc_bin_file,'w') #.temp for build
    group_bin = open(db_prefix_map,'w') #.map
    unique_taxids = set() # for nodes
    bin_length = defaultdict(int) # calculate max bin length
    group_taxid = {} # set group taxid connection for nodes

    for line in stdout.split("\n"):
        if line:
            #acc, length, taxid, [group/rank taxid,] binno
            if use_assembly:
                acc, length, taxid, group, binno = line.split("\t")
                group_taxid[group] = taxid
            else:
                acc, length, taxid, binno = line.split("\t")
                group = taxid # taxid is the classification group

            # if sequences are fragmentes
            if fragment_length: 
                acc, coord = acc.split("/")
                frag_start, frag_end = coord.split(":")
            else:
                frag_start = 1
                frag_end = length

            bin_length[binno]+=int(length)
            unique_taxids.add(taxid)

            print(line, file=bins) 
            print(acc, frag_start, frag_end, binno, sep="\t", file=acc_bin)
            print(group, binno, sep="\t", file=group_bin)
    bins.close()
    acc_bin.close()
    group_bin.close()
    return len(bin_length), unique_taxids, max(bin_length.values()), group_taxid

def get_taxdump(tmp_output_folder):
    tx = time.time()
    sys.stderr.write("Downloading taxdump... ")
    taxdump_file = tmp_output_folder+'taxdump.tar.gz'
    run_wget_taxdump_cmd = 'wget -qO {0} ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz'.format(taxdump_file)
    stdout, stderr, errcode = run(run_wget_taxdump_cmd, print_stderr=True)
    unpack_taxdump_cmd = 'tar xf {0} -C "{1}" nodes.dmp merged.dmp names.dmp'.format(taxdump_file, tmp_output_folder)
    stdout, stderr, errcode = run(unpack_taxdump_cmd, print_stderr=True)
    sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")
    return tmp_output_folder+'nodes.dmp', tmp_output_folder+'merged.dmp', tmp_output_folder+'names.dmp'

def retrieve_ncbi(tmp_output_folder, files, threads, taxsbp_path, use_assembly):
    tx = time.time()
    accessions_file = tmp_output_folder + 'accessions.txt'
    sys.stderr.write("Extracting accessions... ")
    run_get_header = 'zgrep -hoP "(?<=^>)[^ ]+" {0}'.format(" ".join(files))
    stdout, stderr, errcode = run(run_get_header, output_file=accessions_file, print_stderr=False)
    sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")

    tx = time.time()
    sys.stderr.write("Retrieving data from NCBI... ")
    taxsbp_input_file = tmp_output_folder + 'acc_len_taxid.txt'
    run_get_len_taxid_cmd = '{0}get_len_taxid.sh -i {1} {2}'.format(
                            taxsbp_path+"/scripts/",
                            accessions_file,
                            "-a" if use_assembly else "")
    _, stderr, errcode = run(run_get_len_taxid_cmd, output_file=taxsbp_input_file, print_stderr=True)
    sys.stderr.write("Done. Elapsed time: " + str("%.2f" % (time.time() - tx)) + " seconds.\n")
    return taxsbp_input_file

def build_filtered_nodes(unique_taxids, group_taxid, ncbi_nodes_file, ncbi_names_file, use_assembly):
    
    # filtered_nodes = (parent_taxid, name, rank)
    ncbi_nodes, ncbi_ranks = read_nodes(ncbi_nodes_file)
    ncbi_names = read_names(ncbi_names_file)
    filtered_nodes = {}

    # if using assembly, add group to nodes
    if use_assembly:
        for group,taxid in group_taxid.items():
            filtered_nodes[group] = (taxid, group, "assembly")

    # Filter nodes for used taxids
    for leaf_taxid in unique_taxids:
        t = leaf_taxid
        while t!="0":
            if t in filtered_nodes: break # branch already in the dict
            filtered_nodes[t] = (ncbi_nodes[t],ncbi_names[t],ncbi_ranks[t])
            t = ncbi_nodes[t]

    return filtered_nodes

def read_nodes(nodes_file):
    # READ nodes -> fields (1:TAXID 2:PARENT_TAXID 3:RANK)
    nodes = {}
    ranks = {}
    with open(nodes_file,'r') as fnodes:
        for line in fnodes:
            taxid, parent_taxid, rank, _ = line.split('\t|\t',3)
            nodes[taxid] = parent_taxid
            ranks[taxid] = rank
    nodes["1"] = "0"
    return nodes, ranks

def read_names(names_file):
    # READ names -> fields (1:TAXID 2:NAME 3:UNIQUE NAME 4:NAME CLASS)
    names = {}
    with open(names_file,'r') as fnames:
        for line in fnames:
            taxid, name, _, name_class = line.split('\t|\t')
            if name_class.replace('\t|\n','')=="scientific name":
                names[taxid] = name
    return names

def taxid_rank_up_to(taxid, filtered_nodes, fixed_ranks):
    if taxid!="0":
        original_rank = filtered_nodes[taxid][2]
        original_taxid = taxid
        while taxid!="0":
            if(filtered_nodes[taxid][2] in fixed_ranks):
                #everything below species (not being assembly) is counted as species+
                if "species+" in fixed_ranks and original_rank!="species" and original_rank!="assembly" and filtered_nodes[taxid][2]=="species":
                    return original_taxid, "species+"
                else:
                    return taxid, filtered_nodes[taxid][2]
            taxid = filtered_nodes[taxid][0]
        return "1", "root" #no standard rank identified
    else:
        return "0", ""

def parse_db_prefix_nodes(file):
    info = pickle.load(open(file, "rb"))
    kmer_size = info['kmer_size']
    hash_functions = info['hash_functions']
    number_of_bins = info['number_of_bins']
    rank = info['rank']
    bin_length = info['bin_length']
    fragment_length = info['fragment_length']
    overlap_length = info['overlap_length']
    filtered_nodes = info['filtered_nodes']
    return kmer_size, hash_functions, number_of_bins, rank, bin_length, fragment_length, overlap_length, filtered_nodes

def print_final_report(reports, merged_filtered_nodes, seq_unc, total_reads, final_report_file, ranks):

    if not ranks:  
        all_ranks = False
        fixed_ranks = ['root','superkingdom','phylum','class','order','family','genus','species','species+','assembly']
    elif ranks[0]=="all":
        all_ranks = True  
        fixed_ranks = []
    else:
        all_ranks = False
        fixed_ranks = ['root'] + ranks

    # merge nodes
    all_filtered_nodes = {}
    for m in merged_filtered_nodes.values():
        all_filtered_nodes.update(m)

    # sum counts of each report
    merged_rep = defaultdict(int)
    for rep in reports.values():
        for leaf in rep.keys():
            merged_rep[leaf]+=rep[leaf]['count']

    final_rep = defaultdict(lambda: {'count': 0, 'rank': ""})
    
    # make cummulative sum of the counts on the lineage
    for leaf in merged_rep.keys():
        count = merged_rep[leaf]
        if all_ranks: # use all nodes on the tree
            t = leaf
            r = all_filtered_nodes[t][2]
        else: # use only nodes of the fixed ranks
            t, r = taxid_rank_up_to(leaf, all_filtered_nodes, fixed_ranks)

        while t!="0":
            final_rep[t]['count']+=count
            final_rep[t]['rank']=r
            if all_ranks:
                t = all_filtered_nodes[t][0]
                r = all_filtered_nodes[t][2] if t!="0" else ""
            else:
                t, r = taxid_rank_up_to(all_filtered_nodes[t][0], all_filtered_nodes, fixed_ranks)

    # build lineage after all entries were defined
    lineage = {}
    for assignment in final_rep.keys():
        lineage[assignment]=[]
        if all_ranks:
            t=assignment
            while t!="0":
                lineage[assignment].insert(0,t)
                t = all_filtered_nodes[t][0]
        else:
            t, r = taxid_rank_up_to(assignment, all_filtered_nodes, fixed_ranks)
            max_rank_idx = fixed_ranks.index(r) # get index of current rank
            while t!="0":
                # Add empty || if fixed rank is missing
                for i in range(max_rank_idx-fixed_ranks.index(r)):
                    lineage[assignment].insert(0,"")
                    max_rank_idx-=1

                lineage[assignment].insert(0,t)
                max_rank_idx-=1
                t, r = taxid_rank_up_to(all_filtered_nodes[t][0], all_filtered_nodes, fixed_ranks)

    with open(final_report_file, 'w') as frfile:
        frfile.write("unclassified" +"\t"+ "-" +"\t"+ "-" +"\t"+ "-" +"\t"+ str(seq_unc) +"\t"+ str("%.5f" % ((seq_unc/total_reads)*100)) + "\n")
        if all_ranks:
            for assignment in sorted(lineage, key=lineage.get):
                frfile.write(final_rep[assignment]['rank'] +"\t"+ assignment +"\t"+ "|".join(lineage[assignment]) +"\t"+ all_filtered_nodes[assignment][1] +"\t"+ str(final_rep[assignment]['count']) +"\t"+ str("%.5f" % ((final_rep[assignment]['count']/total_reads)*100)) + "\n")
        else:
            for assignment in sorted(lineage, key=lambda k: (fixed_ranks.index(final_rep[k]['rank']), -final_rep[k]['count']), reverse=False):
                frfile.write(final_rep[assignment]['rank'] +"\t"+ assignment +"\t"+ "|".join(lineage[assignment]) +"\t"+ all_filtered_nodes[assignment][1] +"\t"+ str(final_rep[assignment]['count']) +"\t"+ str("%.5f" % ((final_rep[assignment]['count']/total_reads)*100)) + "\n")

if __name__ == '__main__':
    main()
